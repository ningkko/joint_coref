{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import pickle as cPickle\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for pack in os.listdir(\"src\"):\n",
    "    sys.path.append(os.path.join(\"src\", pack))\n",
    "\n",
    "sys.path.append(\"/src/shared/\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from swirl_parsing import parse_swirl_output\n",
    "from allen_srl_reader import read_srl\n",
    "from create_bert_embeddings import *\n",
    "from create_elmo_embeddings import *\n",
    "from classes import Document, Sentence, Token, EventMention, EntityMention\n",
    "from extraction_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"build_features_config.json\"\n",
    "output_path=\"data/feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open(config_path, 'r') as js_file:\n",
    "    config_dict = json.load(js_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_mentions_from_json(mentions_json_file, docs, is_event, is_gold_mentions):\n",
    "    '''\n",
    "    Loading mentions from JSON file and add those to the documents objects\n",
    "    :param mentions_json_file: the JSON file contains the mentions\n",
    "    :param docs:  set of document objects\n",
    "    :param is_event: a boolean indicates whether the function extracts event or entity mentions\n",
    "    :param is_gold_mentions: a boolean indicates whether the function extracts gold or predicted\n",
    "    mentions\n",
    "    '''\n",
    "    with open(mentions_json_file, 'r') as js_file:\n",
    "        js_mentions = json.load(js_file)\n",
    "\n",
    "    for js_mention in js_mentions:\n",
    "        doc_id = js_mention[\"doc_id\"].replace('.xml', '')\n",
    "        sent_id = js_mention[\"sent_id\"]\n",
    "        tokens_numbers = js_mention[\"tokens_number\"]\n",
    "        mention_type = js_mention[\"mention_type\"]\n",
    "        is_singleton = js_mention[\"is_singleton\"]\n",
    "        is_continuous = js_mention[\"is_continuous\"]\n",
    "        mention_str = js_mention[\"tokens_str\"]\n",
    "        coref_chain = js_mention[\"coref_chain\"]\n",
    "        if mention_str is None:\n",
    "            print(js_mention)\n",
    "        head_text, head_lemma = find_head(mention_str)\n",
    "        score = js_mention[\"score\"]\n",
    "        try:\n",
    "            token_objects = docs[doc_id].get_sentences()[sent_id].find_mention_tokens(tokens_numbers)\n",
    "        except:\n",
    "            print('error when looking for mention tokens')\n",
    "            print('doc id {} sent id {}'.format(doc_id, sent_id))\n",
    "            print('token numbers - {}'.format(str(tokens_numbers)))\n",
    "            print('mention string {}'.format(mention_str))\n",
    "            print('sentence - {}'.format(docs[doc_id].get_sentences()[sent_id].get_raw_sentence()))\n",
    "            raise\n",
    "\n",
    "        # Sanity check - check if all mention's tokens can be found\n",
    "        if not token_objects:\n",
    "            print('Can not find tokens of a mention - {} {} {}'.format(doc_id, sent_id,tokens_numbers))\n",
    "\n",
    "        # Mark the mention's gold coref chain in its tokens\n",
    "        if is_gold_mentions:\n",
    "            for token in token_objects:\n",
    "                if is_event:\n",
    "                    token.gold_event_coref_chain.append(coref_chain)\n",
    "                else:\n",
    "                    token.gold_entity_coref_chain.append(coref_chain)\n",
    "\n",
    "        if is_event:\n",
    "            mention = EventMention(doc_id, sent_id, tokens_numbers,token_objects,mention_str, head_text,\n",
    "                                   head_lemma, is_singleton, is_continuous, coref_chain)\n",
    "        else:\n",
    "            mention = EntityMention(doc_id, sent_id, tokens_numbers,token_objects, mention_str, head_text,\n",
    "                                    head_lemma, is_singleton, is_continuous, coref_chain, mention_type)\n",
    "\n",
    "        mention.probability = score  # a confidence score for predicted mentions (if used), set gold mentions prob to 1.0\n",
    "        if is_gold_mentions:\n",
    "            docs[doc_id].get_sentences()[sent_id].add_gold_mention(mention, is_event)\n",
    "        else:\n",
    "            docs[doc_id].get_sentences()[sent_id]. \\\n",
    "                add_predicted_mention(mention, is_event,\n",
    "                                      relaxed_match=config_dict[\"relaxed_match_with_gold_mention\"])\n",
    "\n",
    "\n",
    "def load_gold_mentions(docs,events_json, entities_json):\n",
    "    '''\n",
    "    A function loads gold event and entity mentions\n",
    "    :param docs: set of document objects\n",
    "    :param events_json:  a JSON file contains the gold event mentions (of a specific split - train/dev/test)\n",
    "    :param entities_json: a JSON file contains the gold entity mentions (of a specific split - train/dev/test)\n",
    "    '''\n",
    "    load_mentions_from_json(events_json,docs,is_event=True, is_gold_mentions=True)\n",
    "    load_mentions_from_json(entities_json,docs,is_event=False, is_gold_mentions=True)\n",
    "\n",
    "\n",
    "def load_predicted_mentions(docs,events_json, entities_json):\n",
    "    '''\n",
    "    This function loads predicted event and entity mentions\n",
    "    :param docs: set of document objects\n",
    "    :param events_json:  a JSON file contains predicted event mentions (of a specific split - train/dev/test)\n",
    "    :param entities_json: a JSON file contains predicted entity mentions (of a specific split - train/dev/test)\n",
    "    '''\n",
    "    load_mentions_from_json(events_json,docs,is_event=True, is_gold_mentions=False)\n",
    "    load_mentions_from_json(entities_json,docs,is_event=False, is_gold_mentions=False)\n",
    "\n",
    "\n",
    "def load_gold_data(split_txt_file, events_json, entities_json):\n",
    "    '''\n",
    "    This function loads the texts of each split and its gold mentions, create document objects\n",
    "    and stored the gold mentions within their suitable document objects\n",
    "    :param split_txt_file: the text file of each split is written as 5 columns (stored in data/intermid)\n",
    "    :param events_json: a JSON file contains the gold event mentions\n",
    "    :param entities_json: a JSON file contains the gold event mentions\n",
    "    :return:\n",
    "    '''\n",
    "    logger.info('Loading gold mentions...')\n",
    "    docs = load_ECB_plus(split_txt_file)\n",
    "    load_gold_mentions(docs, events_json, entities_json)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def load_predicted_data(docs, pred_events_json, pred_entities_json):\n",
    "    '''\n",
    "    This function loads the predicted mentions and stored them within their suitable document objects\n",
    "    (suitable for loading the test data)\n",
    "    :param docs: dictionary that contains the document objects\n",
    "    :param pred_events_json: a JSON file contains predicted event mentions\n",
    "    :param pred_entities_json: a JSON file contains predicted event mentions\n",
    "    :return:\n",
    "    '''\n",
    "    logger.info('Loading predicted mentions...')\n",
    "    load_predicted_mentions(docs, pred_events_json, pred_entities_json)\n",
    "\n",
    "\n",
    "def find_head(x):\n",
    "    '''\n",
    "    This function finds the head and head lemma of a mention x\n",
    "    :param x: A mention object\n",
    "    :return: the head word and\n",
    "    '''\n",
    "\n",
    "    x_parsed = nlp(x)\n",
    "    for tok in x_parsed:\n",
    "        if tok.head == tok:\n",
    "            if tok.lemma_ == u'-PRON-':\n",
    "                return tok.text, tok.text.lower()\n",
    "            return tok.text,tok.lemma_\n",
    "\n",
    "\n",
    "def have_string_match(mention,arg_str ,arg_start, arg_end):\n",
    "    '''\n",
    "    This function checks whether a given entity mention has a string match (strict or relaxed)\n",
    "    with a span of an extracted argument\n",
    "    :param mention: a candidate entity mention\n",
    "    :param arg_str: the argument's text\n",
    "    :param arg_start: the start index of the argument's span\n",
    "    :param arg_end: the end index of the argument's span\n",
    "    :return: True if there is a string match (strict or relaxed) between the entity mention\n",
    "    and the extracted argument's span, and false otherwise\n",
    "    '''\n",
    "    if mention.mention_str == arg_str and mention.start_offset == arg_start:  # exact string match + same start index\n",
    "        return True\n",
    "    if mention.mention_str == arg_str:  # exact string match\n",
    "        return True\n",
    "    if mention.start_offset >= arg_start and mention.end_offset <= arg_end:  # the argument span contains the mention span\n",
    "        return True\n",
    "    if arg_start >= mention.start_offset and arg_end <= mention.end_offset:  # the mention span contains the mention span\n",
    "        return True\n",
    "    if len(set(mention.tokens_numbers).intersection(set(range(arg_start,arg_end + 1)))) > 0: # intersection between the mention's tokens and the argument's tokens\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def add_arg_to_event(entity, event, rel_name):\n",
    "    '''\n",
    "    Adds the entity mention as an argument (in a specific role) of an event mention and also adds the\n",
    "    event mention as predicate (in a specific role) of the entity mention\n",
    "    :param entity: an entity mention object\n",
    "    :param event: an event mention object\n",
    "    :param rel_name: the specific role\n",
    "    '''\n",
    "    if rel_name == 'A0':\n",
    "        event.arg0 = (entity.mention_str, entity.mention_id)\n",
    "        entity.add_predicate((event.mention_str, event.mention_id), 'A0')\n",
    "    elif rel_name == 'A1':\n",
    "        event.arg1 = (entity.mention_str, entity.mention_id)\n",
    "        entity.add_predicate((event.mention_str, event.mention_id), 'A1')\n",
    "    elif rel_name == 'AM-TMP':\n",
    "        event.amtmp = (entity.mention_str, entity.mention_id)\n",
    "        entity.add_predicate((event.mention_str, event.mention_id), 'AM-TMP')\n",
    "    elif rel_name == 'AM-LOC':\n",
    "        event.amloc = (entity.mention_str, entity.mention_id)\n",
    "        entity.add_predicate((event.mention_str, event.mention_id), 'AM-LOC')\n",
    "\n",
    "\n",
    "def find_argument(rel_name, rel_tokens, matched_event, sent_entities, sent_obj, is_gold, srl_obj):\n",
    "    '''\n",
    "    This function matches between an argument of an event mention and an entity mention.\n",
    "    :param rel_name: the specific role of the argument\n",
    "    :param rel_tokens: the argument's tokens\n",
    "    :param matched_event: the event mention\n",
    "    :param sent_entities: a entity mentions exist in the event's sentence.\n",
    "    :param sent_obj: the object represents the sentence\n",
    "    :param is_gold: whether the argument need to be matched with a gold mention or not\n",
    "    :param srl_obj: an object represents the extracted SRL argument.\n",
    "    :return True if the extracted SRL argument was matched with an entity mention.\n",
    "    '''\n",
    "    arg_start_ix = rel_tokens[0]\n",
    "    if len(rel_tokens) > 1:\n",
    "        arg_end_ix = rel_tokens[1]\n",
    "    else:\n",
    "        arg_end_ix = rel_tokens[0]\n",
    "\n",
    "    if arg_end_ix >= len(sent_obj.get_tokens()):\n",
    "        print('argument bound mismatch with sentence length')\n",
    "        print('arg start index - {}'.format(arg_start_ix))\n",
    "        print('arg end index - {}'.format(arg_end_ix))\n",
    "        print('sentence length - {}'.format(len(sent_obj.get_tokens())))\n",
    "        print('raw sentence: {}'.format(sent_obj.get_raw_sentence()))\n",
    "        print('matched event: {}'.format(str(matched_event)))\n",
    "        print('srl obj - {}'.format(str(srl_obj)))\n",
    "\n",
    "    arg_str, arg_tokens = sent_obj.fetch_mention_string(arg_start_ix, arg_end_ix)\n",
    "\n",
    "    entity_found = False\n",
    "    matched_entity = None\n",
    "    for entity in sent_entities:\n",
    "        if have_string_match(entity, arg_str, arg_start_ix, arg_end_ix):\n",
    "            if rel_name == 'AM-TMP' and entity.mention_type != 'TIM':\n",
    "                continue\n",
    "            if rel_name == 'AM-LOC' and entity.mention_type != 'LOC':\n",
    "                continue\n",
    "            entity_found = True\n",
    "            matched_entity = entity\n",
    "            break\n",
    "    if entity_found:\n",
    "        add_arg_to_event(matched_entity, matched_event, rel_name)\n",
    "        if is_gold:\n",
    "            return True\n",
    "        else:\n",
    "            if matched_entity.gold_mention_id is not None:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def match_allen_srl_structures(dataset, srl_data, is_gold):\n",
    "    '''\n",
    "    Matches between extracted predicates and event mentions and between their arguments and\n",
    "    entity mentions, designed to handle the output of Allen NLP SRL system\n",
    "    :param dataset: an object represents the spilt (train/dev/test)\n",
    "    :param srl_data: a dictionary contains the predicate-argument structures\n",
    "    :param is_gold: whether to match predicate-argument structures with gold mentions or with predicted mentions\n",
    "    '''\n",
    "    matched_events_count = 0\n",
    "    matched_args_count = 0\n",
    "\n",
    "    for topic_id, topic in dataset.topics.items():\n",
    "        for doc_id, doc in topic.docs.items():\n",
    "            for sent_id, sent in doc.get_sentences().items():\n",
    "                # Handling nominalizations in case we don't use syntactic dependencies (which already handle this)\n",
    "                if not config_dict[\"use_dep\"]:\n",
    "                    sent_str = sent.get_raw_sentence()\n",
    "                    parsed_sent = nlp(sent_str)\n",
    "                    find_nominalizations_args(parsed_sent, sent, is_gold)\n",
    "                sent_srl_info = None\n",
    "\n",
    "                if doc_id in srl_data:\n",
    "                    doc_srl = srl_data[doc_id]\n",
    "                    if int(sent_id) in doc_srl:\n",
    "                        sent_srl_info = doc_srl[int(sent_id)]\n",
    "\n",
    "                if sent_srl_info is not None:\n",
    "                    for event_srl in sent_srl_info.srl:\n",
    "                        event_text = event_srl.verb.text\n",
    "                        event_ecb_tok_ids = event_srl.verb.ecb_tok_ids\n",
    "\n",
    "                        if is_gold:\n",
    "                            sent_events = sent.gold_event_mentions\n",
    "                            sent_entities = sent.gold_entity_mentions\n",
    "                        else:\n",
    "                            sent_events = sent.pred_event_mentions\n",
    "                            sent_entities = sent.pred_entity_mentions\n",
    "                        event_found = False\n",
    "                        matched_event = None\n",
    "\n",
    "                        for event_mention in sent_events:\n",
    "                            if event_ecb_tok_ids == event_mention.tokens_numbers or \\\n",
    "                                    event_text == event_mention.mention_str or \\\n",
    "                                    event_text in event_mention.mention_str or \\\n",
    "                                    event_mention.mention_str in event_text:\n",
    "                                event_found = True\n",
    "                                matched_event = event_mention\n",
    "                                if is_gold:\n",
    "                                    matched_events_count += 1\n",
    "                                elif matched_event.gold_mention_id is not None:\n",
    "                                    matched_events_count += 1\n",
    "                            if event_found:\n",
    "                                break\n",
    "                        if event_found:\n",
    "                            if event_srl.arg0 is not None:\n",
    "                                if match_entity_with_srl_argument(sent_entities, matched_event,\n",
    "                                                                  event_srl.arg0, 'A0', is_gold):\n",
    "                                    matched_args_count += 1\n",
    "\n",
    "                            if event_srl.arg1 is not None:\n",
    "                                if match_entity_with_srl_argument(sent_entities, matched_event,\n",
    "                                                                  event_srl.arg1, 'A1', is_gold):\n",
    "                                    matched_args_count += 1\n",
    "                            if event_srl.arg_tmp is not None:\n",
    "                                if match_entity_with_srl_argument(sent_entities, matched_event,\n",
    "                                                                  event_srl.arg_tmp, 'AM-TMP', is_gold):\n",
    "                                    matched_args_count += 1\n",
    "\n",
    "                            if event_srl.arg_loc is not None:\n",
    "                                if match_entity_with_srl_argument(sent_entities, matched_event,\n",
    "                                                                  event_srl.arg_loc, 'AM-LOC', is_gold):\n",
    "                                    matched_args_count += 1\n",
    "\n",
    "    logger.info('SRL matched events - ' + str(matched_events_count))\n",
    "    logger.info('SRL matched args - ' + str(matched_args_count))\n",
    "\n",
    "\n",
    "def match_entity_with_srl_argument(sent_entities, matched_event ,srl_arg,rel_name, is_gold):\n",
    "    '''\n",
    "    This function matches between an argument of an event mention and an entity mention.\n",
    "    Designed to handle the output of Allen NLP SRL system\n",
    "    :param sent_entities: the entity mentions in the event's sentence\n",
    "    :param matched_event: the event mention\n",
    "    :param srl_arg: the extracted argument\n",
    "    :param rel_name: the role name\n",
    "    :param is_gold: whether to match the argument with gold entity mention or with predicted entity mention\n",
    "    :return:\n",
    "    '''\n",
    "    found_entity = False\n",
    "    matched_entity = None\n",
    "    for entity in sent_entities:\n",
    "        if srl_arg.ecb_tok_ids == entity.tokens_numbers or \\\n",
    "                srl_arg.text == entity.mention_str or \\\n",
    "                srl_arg.text in entity.mention_str or \\\n",
    "                entity.mention_str in srl_arg.text:\n",
    "            if rel_name == 'AM-TMP' and entity.mention_type != 'TIM':\n",
    "                continue\n",
    "            if rel_name == 'AM-LOC' and entity.mention_type != 'LOC':\n",
    "                continue\n",
    "            found_entity = True\n",
    "            matched_entity = entity\n",
    "\n",
    "        if found_entity:\n",
    "            break\n",
    "\n",
    "    if found_entity:\n",
    "        add_arg_to_event(matched_entity, matched_event, rel_name)\n",
    "        if is_gold:\n",
    "            return True\n",
    "        else:\n",
    "            if matched_entity.gold_mention_id is not None:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_srl_info(dataset, srl_data, is_gold):\n",
    "    '''\n",
    "    Matches between extracted predicates and event mentions and between their arguments and\n",
    "    entity mentions.\n",
    "    :param dataset: an object represents the spilt (train/dev/test)\n",
    "    :param srl_data: a dictionary contains the predicate-argument structures\n",
    "    :param is_gold: whether to match predicate-argument structures with gold mentions or with predicted mentions\n",
    "    '''\n",
    "    matched_events_count = 0\n",
    "    unmatched_event_count = 0\n",
    "    matched_args_count = 0\n",
    "\n",
    "    matched_identified_events = 0\n",
    "    matched_identified_args = 0\n",
    "    for topic_id, topic in dataset.topics.items():\n",
    "        for doc_id, doc in topic.docs.items():\n",
    "            for sent_id, sent in doc.get_sentences().items():\n",
    "                # Handling nominalizations if we don't use dependency parsing (that already handles it)\n",
    "                if not config_dict[\"use_dep\"]:\n",
    "                    sent_str = sent.get_raw_sentence()\n",
    "                    parsed_sent = nlp(sent_str)\n",
    "                    find_nominalizations_args(parsed_sent, sent, is_gold)\n",
    "                sent_srl_info = {}\n",
    "\n",
    "                if doc_id in srl_data:\n",
    "                    doc_srl = srl_data[doc_id]\n",
    "                    if int(sent_id) in doc_srl:\n",
    "                        sent_srl_info = doc_srl[int(sent_id)]\n",
    "                else:\n",
    "                    print('doc not in srl data - ' + doc_id)\n",
    "\n",
    "                for event_key, srl_obj in sent_srl_info.items():\n",
    "                    if is_gold:\n",
    "                        sent_events = sent.gold_event_mentions\n",
    "                        sent_entities = sent.gold_entity_mentions\n",
    "                    else:\n",
    "                        sent_events = sent.pred_event_mentions\n",
    "                        sent_entities = sent.pred_entity_mentions\n",
    "                    event_found = False\n",
    "                    matched_event = None\n",
    "                    for event_mention in sent_events:\n",
    "                        if event_key in event_mention.tokens_numbers:\n",
    "                            event_found = True\n",
    "                            matched_event = event_mention\n",
    "                            if is_gold:\n",
    "                                matched_events_count += 1\n",
    "                            elif matched_event.gold_mention_id is not None:\n",
    "                                    matched_events_count += 1\n",
    "                        if event_found:\n",
    "                            break\n",
    "                    if event_found:\n",
    "                        for rel_name, rel_tokens in srl_obj.arg_info.items():\n",
    "                            if find_argument(rel_name, rel_tokens, matched_event, sent_entities, sent, is_gold,srl_obj):\n",
    "                                matched_args_count += 1\n",
    "                    else:\n",
    "                        unmatched_event_count += 1\n",
    "    logger.info('SRL matched events - ' + str(matched_events_count))\n",
    "    logger.info('SRL unmatched events - ' + str(unmatched_event_count))\n",
    "    logger.info('SRL matched args - ' + str(matched_args_count))\n",
    "\n",
    "\n",
    "def find_topic_gold_clusters(topic):\n",
    "    '''\n",
    "    Finds the gold clusters of a specific topic\n",
    "    :param topic: a topic object\n",
    "    :return: a mapping of coref chain to gold cluster (for a specific topic) and the topic's mentions\n",
    "    '''\n",
    "    event_mentions = []\n",
    "    entity_mentions = []\n",
    "    # event_gold_tag_to_cluster = defaultdict(list)\n",
    "    # entity_gold_tag_to_cluster = defaultdict(list)\n",
    "\n",
    "    event_gold_tag_to_cluster = {}\n",
    "    entity_gold_tag_to_cluster = {}\n",
    "\n",
    "    for doc_id, doc in topic.docs.items():\n",
    "        for sent_id, sent in doc.sentences.items():\n",
    "            event_mentions.extend(sent.gold_event_mentions)\n",
    "            entity_mentions.extend(sent.gold_entity_mentions)\n",
    "\n",
    "    for event in event_mentions:\n",
    "        if event.gold_tag != '-':\n",
    "            if event.gold_tag not in event_gold_tag_to_cluster:\n",
    "                event_gold_tag_to_cluster[event.gold_tag] = []\n",
    "            event_gold_tag_to_cluster[event.gold_tag].append(event)\n",
    "    for entity in entity_mentions:\n",
    "        if entity.gold_tag != '-':\n",
    "            if entity.gold_tag not in entity_gold_tag_to_cluster:\n",
    "                entity_gold_tag_to_cluster[entity.gold_tag] = []\n",
    "            entity_gold_tag_to_cluster[entity.gold_tag].append(entity)\n",
    "\n",
    "    return event_gold_tag_to_cluster, entity_gold_tag_to_cluster, event_mentions, entity_mentions\n",
    "\n",
    "\n",
    "def write_dataset_statistics(split_name, dataset, check_predicted):\n",
    "    '''\n",
    "    Prints the split statistics\n",
    "    :param split_name: the split name (a string)\n",
    "    :param dataset: an object represents the split\n",
    "    :param check_predicted: whether to print statistics of predicted mentions too\n",
    "    '''\n",
    "    docs_count = 0\n",
    "    sent_count = 0\n",
    "    event_mentions_count = 0\n",
    "    entity_mentions_count = 0\n",
    "    event_chains_count = 0\n",
    "    entity_chains_count = 0\n",
    "    topics_count = len(dataset.topics.keys())\n",
    "    predicted_events_count = 0\n",
    "    predicted_entities_count = 0\n",
    "    matched_predicted_event_count = 0\n",
    "    matched_predicted_entity_count = 0\n",
    "\n",
    "\n",
    "    for topic_id, topic in dataset.topics.items():\n",
    "        event_gold_tag_to_cluster, entity_gold_tag_to_cluster, \\\n",
    "        event_mentions, entity_mentions = find_topic_gold_clusters(topic)\n",
    "\n",
    "        docs_count += len(topic.docs.keys())\n",
    "        sent_count += sum([len(doc.sentences.keys()) for doc_id, doc in topic.docs.items()])\n",
    "        event_mentions_count += len(event_mentions)\n",
    "        entity_mentions_count += len(entity_mentions)\n",
    "\n",
    "        entity_chains = set()\n",
    "        event_chains = set()\n",
    "\n",
    "        for mention in entity_mentions:\n",
    "            entity_chains.add(mention.gold_tag)\n",
    "\n",
    "        for mention in event_mentions:\n",
    "            event_chains.add(mention.gold_tag)\n",
    "\n",
    "        # event_chains_count += len(set(event_gold_tag_to_cluster.keys()))\n",
    "        # entity_chains_count += len(set(entity_gold_tag_to_cluster.keys()))\n",
    "\n",
    "        event_chains_count += len(event_chains)\n",
    "        entity_chains_count += len(entity_chains)\n",
    "\n",
    "        if check_predicted:\n",
    "            for doc_id, doc in topic.docs.items():\n",
    "                for sent_id, sent in doc.sentences.items():\n",
    "                    pred_events = sent.pred_event_mentions\n",
    "                    pred_entities = sent.pred_entity_mentions\n",
    "\n",
    "                    predicted_events_count += len(pred_events)\n",
    "                    predicted_entities_count += len(pred_entities)\n",
    "\n",
    "                    for pred_event in pred_events:\n",
    "                        if pred_event.has_compatible_mention:\n",
    "                            matched_predicted_event_count += 1\n",
    "\n",
    "                    for pred_entity in pred_entities:\n",
    "                        if pred_entity.has_compatible_mention:\n",
    "                            matched_predicted_entity_count += 1\n",
    "\n",
    "    with open(os.path.join(args.output_path, '{}_statistics.txt'.format(split_name)), 'w') as f:\n",
    "        f.write('Number of topics - {}\\n'.format(topics_count))\n",
    "        f.write('Number of documents - {}\\n'.format(docs_count))\n",
    "        f.write('Number of sentences - {}\\n'.format(sent_count))\n",
    "        f.write('Number of event mentions - {}\\n'.format(event_mentions_count))\n",
    "        f.write('Number of entity mentions - {}\\n'.format(entity_mentions_count))\n",
    "\n",
    "        if check_predicted:\n",
    "            f.write('Number of predicted event mentions  - {}\\n'.format(predicted_events_count))\n",
    "            f.write('Number of predicted entity mentions - {}\\n'.format(predicted_entities_count))\n",
    "            f.write('Number of predicted event mentions that match gold mentions- '\n",
    "                    '{} ({}%)\\n'.format(matched_predicted_event_count,\n",
    "                                        (matched_predicted_event_count/float(event_mentions_count)) *100 ))\n",
    "            f.write('Number of predicted entity mentions that match gold mentions- '\n",
    "                    '{} ({}%)\\n'.format(matched_predicted_entity_count,\n",
    "                                        (matched_predicted_entity_count / float(entity_mentions_count)) * 100))\n",
    "\n",
    "\n",
    "def obj_dict(obj):\n",
    "    obj_d = obj.__dict__\n",
    "    obj_d = stringify_keys(obj_d)\n",
    "    return obj_d\n",
    "\n",
    "\n",
    "def stringify_keys(d):\n",
    "    \"\"\"Convert a dict's keys to strings if they are not.\"\"\"\n",
    "    for key in d.keys():\n",
    "\n",
    "        # check inner dict\n",
    "        if isinstance(d[key], dict):\n",
    "            value = stringify_keys(d[key])\n",
    "        else:\n",
    "            value = d[key]\n",
    "\n",
    "        # convert nonstring to string if needed\n",
    "        if not isinstance(key, str):\n",
    "            try:\n",
    "                d[str(key)] = value\n",
    "            except Exception:\n",
    "                try:\n",
    "                    d[repr(key)] = value\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # delete old key\n",
    "            del d[key]\n",
    "    return d\n",
    "\n",
    "\n",
    "def set_embed_to_mention(mention, sent_embeddings):\n",
    "    '''\n",
    "    Sets the Bert embeddings of a mention\n",
    "    :param mention: event/entity mention object\n",
    "    :param sent_embeddings: the embedding for each word in the sentence produced by Bert model\n",
    "    :return:\n",
    "    '''\n",
    "    print()\n",
    "    head_index = int(mention.get_head_index())+1     # Bert embedding has pads\n",
    "    print(head_index)\n",
    "    print(len(sent_embeddings))\n",
    "    head_embeddings = sent_embeddings[head_index]\n",
    "    mention.head_bert_embeddings = head_embeddings\n",
    "\n",
    "\n",
    "def set_embeddings_to_mentions(embedder, sentence, set_pred_mentions):\n",
    "    '''\n",
    "     Sets the ELMo embeddings for all the mentions in the sentence\n",
    "    :param embedder: a wrapper object\n",
    "    :param sentence: a sentence object\n",
    "    '''\n",
    "    embedding = embedder.get_embedding(sentence)\n",
    "    event_mentions = sentence.gold_event_mentions\n",
    "    entity_mentions = sentence.gold_entity_mentions\n",
    "\n",
    "    for event in event_mentions:\n",
    "        set_embed_to_mention(event, embedding)\n",
    "    for entity in entity_mentions:\n",
    "        set_embed_to_mention(entity, embedding)\n",
    "\n",
    "    # Set the contextualized vector also for predicted mentions\n",
    "    if set_pred_mentions:\n",
    "        for event in sentence.pred_event_mentions:\n",
    "            set_embed_to_mention(event, embedding)  # set the head contextualized vector\n",
    "\n",
    "        for entity in sentence.pred_entity_mentions:\n",
    "            set_embed_to_mention(entity, embedding)  # set the head contextualized vector\n",
    "\n",
    "\n",
    "def load_embeddings(dataset, embedder, set_pred_mentions):\n",
    "    '''\n",
    "    Sets the ELMo embeddings for all the mentions in the split\n",
    "    :param dataset: an object represents a split (train/dev/test)\n",
    "    :param embedder: a wrapper object\n",
    "    :return:\n",
    "    '''\n",
    "    for topic_id, topic in dataset.topics.items():\n",
    "        for doc_id, doc in topic.docs.items():\n",
    "            for sent_id, sent in doc.get_sentences().items():\n",
    "                set_embeddings_to_mentions(embedder, sent, set_pred_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_gold_data(config_dict[\"train_text_file\"],config_dict[\"train_event_mentions\"],\n",
    "                               config_dict[\"train_entity_mentions\"])\n",
    "train_set = order_docs_by_topics(training_data)\n",
    "topic_id, topic = list(train_set.topics.items())[0]\n",
    "_, doc = list(topic.docs.items())[0]\n",
    "_, sent = list(doc.get_sentences().items())[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "elmo = ElmoEmbedder(config_dict[\"options_file\"], config_dict[\"weight_file\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = \" The  Mean Girls  star fooled an L . A Superior Court judge , a Santa Monica prosecutor and apparently her own lawyer , by pretending to check into a rehabilitation facility , but chickening out when she got there .\".split(\" \")\n",
    "embeddings = elmo.embed_sentence(tokenized_sent)\n",
    "output = np.average(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The',\n",
       " '',\n",
       " 'Mean',\n",
       " 'Girls',\n",
       " '',\n",
       " 'star',\n",
       " 'fooled',\n",
       " 'an',\n",
       " 'L',\n",
       " '.',\n",
       " 'A',\n",
       " 'Superior',\n",
       " 'Court',\n",
       " 'judge',\n",
       " ',',\n",
       " 'a',\n",
       " 'Santa',\n",
       " 'Monica',\n",
       " 'prosecutor',\n",
       " 'and',\n",
       " 'apparently',\n",
       " 'her',\n",
       " 'own',\n",
       " 'lawyer',\n",
       " ',',\n",
       " 'by',\n",
       " 'pretending',\n",
       " 'to',\n",
       " 'check',\n",
       " 'into',\n",
       " 'a',\n",
       " 'rehabilitation',\n",
       " 'facility',\n",
       " ',',\n",
       " 'but',\n",
       " 'chickening',\n",
       " 'out',\n",
       " 'when',\n",
       " 'she',\n",
       " 'got',\n",
       " 'there',\n",
       " '.']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perennial party girl Tara Reid checked herself into Promises Treatment Center , her rep told People .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sent.get_raw_sentence()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base',\n",
    "                                     output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" The  Mean Girls  star fooled an L . A Superior Court judge , a Santa Monica prosecutor and apparently her own lawyer , by pretending to check into a rehabilitation facility , but chickening out when she got there .\"\n",
    "# text = \"Perennial party girl Tara Reid checked herself into Promises Treatment Center , her rep told People .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The             20\n",
      "             1,437\n",
      "Mean        30,750\n",
      "Girls        7,707\n",
      "             1,437\n",
      "star           999\n",
      "fooled      31,952\n",
      "an              41\n",
      "L              226\n",
      ".              479\n",
      "A               83\n",
      "Superior    11,486\n",
      "Court          837\n",
      "judge        1,679\n",
      ",            2,156\n",
      "a               10\n",
      "Santa        2,005\n",
      "Monica      12,811\n",
      "prosecutor   5,644\n",
      "and              8\n",
      "apparently   4,100\n",
      "her             69\n",
      "own            308\n",
      "lawyer       2,470\n",
      ",            2,156\n",
      "by              30\n",
      "pretending  23,748\n",
      "to               7\n",
      "check        1,649\n",
      "into            88\n",
      "a               10\n",
      "rehabilitation 11,226\n",
      "facility     2,122\n",
      ",            2,156\n",
      "but             53\n",
      "chick       30,802\n",
      "ening         4,226\n",
      "out             66\n",
      "when            77\n",
      "she             79\n",
      "got            300\n",
      "there           89\n",
      ".              479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    outputs = model(tokens_tensor, segments_tensors) \n",
    "    hidden_states = outputs[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 43\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 43, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings, dim=1) \n",
    "token_embeddings = token_embeddings.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs_sum = []\n",
    "for token in token_embeddings:\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)       # Sum the vectors from the last four layers.\n",
    "    token_vecs_sum.append(sum_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_vecs_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '',\n",
       " 'Mean',\n",
       " 'Girls',\n",
       " '',\n",
       " 'star',\n",
       " 'fooled',\n",
       " 'an',\n",
       " 'L',\n",
       " '.',\n",
       " 'A',\n",
       " 'Superior',\n",
       " 'Court',\n",
       " 'judge',\n",
       " ',',\n",
       " 'a',\n",
       " 'Santa',\n",
       " 'Monica',\n",
       " 'prosecutor',\n",
       " 'and',\n",
       " 'apparently',\n",
       " 'her',\n",
       " 'own',\n",
       " 'lawyer',\n",
       " ',',\n",
       " 'by',\n",
       " 'pretending',\n",
       " 'to',\n",
       " 'check',\n",
       " 'into',\n",
       " 'a',\n",
       " 'rehabilitation',\n",
       " 'facility',\n",
       " ',',\n",
       " 'but',\n",
       " 'chick',\n",
       " 'ening',\n",
       " 'out',\n",
       " 'when',\n",
       " 'she',\n",
       " 'got',\n",
       " 'there',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0],\n",
       " [1],\n",
       " [2],\n",
       " [3],\n",
       " [4],\n",
       " [5],\n",
       " [6],\n",
       " [7],\n",
       " [8],\n",
       " [9],\n",
       " [10],\n",
       " [11],\n",
       " [12],\n",
       " [13],\n",
       " [14],\n",
       " [15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [20],\n",
       " [21],\n",
       " [22],\n",
       " [23],\n",
       " [24],\n",
       " [25],\n",
       " [26],\n",
       " [27],\n",
       " [28],\n",
       " [29],\n",
       " [30],\n",
       " [31],\n",
       " [32],\n",
       " [33],\n",
       " [34],\n",
       " [35, 36],\n",
       " [37],\n",
       " [38],\n",
       " [39],\n",
       " [40],\n",
       " [41],\n",
       " [42]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_token_index(tokenized_text): \n",
    "\n",
    "    output = []\n",
    "    current_word = []\n",
    "\n",
    "    for i in range(0,len(tokenized_text)):\n",
    "        if \"\" in tokenized_text[i]:\n",
    "            if current_word:\n",
    "                output.append(current_word)     # store the previous word\n",
    "            current_word = [i]              # start documenting the current one\n",
    "        else:\n",
    "            current_word.append(i)\n",
    "    output.append(current_word)      # the last word was not handled in the loop\n",
    "    \n",
    "    return output\n",
    "\n",
    "find_token_index(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_embedding(embeddings):\n",
    "    '''\n",
    "    :param embeddings: a list of embeddings\n",
    "    '''\n",
    "    arrays = [np.array(x) for x in embeddings]\n",
    "    return [np.mean(k) for k in zip(*arrays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6622)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a sannity check\n",
    "v = 0\n",
    "for i in range(0,3):\n",
    "    v += token_vecs_sum[i][0]\n",
    "v/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.662164,\n",
       " -1.4150788,\n",
       " 0.8553061,\n",
       " 0.38283196,\n",
       " 4.3057246,\n",
       " 1.1405891,\n",
       " -0.26366517,\n",
       " -1.3919495,\n",
       " 0.21666677,\n",
       " 0.62895155,\n",
       " -0.3246144,\n",
       " -0.6538248,\n",
       " -0.5340648,\n",
       " 1.3044595,\n",
       " -0.3286626,\n",
       " 0.01206094,\n",
       " -1.1887469,\n",
       " 0.39108312,\n",
       " -0.7243746,\n",
       " 0.14947085,\n",
       " 1.1596373,\n",
       " 0.20583956,\n",
       " -0.60892516,\n",
       " 0.41969028,\n",
       " 1.2827133,\n",
       " -1.192368,\n",
       " -0.23067756,\n",
       " 0.8322663,\n",
       " 0.6761567,\n",
       " -0.86581564,\n",
       " -0.24033599,\n",
       " 0.86172646,\n",
       " 0.04737252,\n",
       " -0.7100203,\n",
       " -0.16471626,\n",
       " -0.42223385,\n",
       " 0.831733,\n",
       " -0.3565837,\n",
       " 0.71448237,\n",
       " 1.3269125,\n",
       " 2.0936773,\n",
       " 0.44272256,\n",
       " -0.035212833,\n",
       " -0.7429903,\n",
       " 0.61380476,\n",
       " -0.3511108,\n",
       " 0.60613525,\n",
       " 1.5029255,\n",
       " -0.31069633,\n",
       " 0.39110747,\n",
       " 0.36468625,\n",
       " -0.17605019,\n",
       " 0.5785273,\n",
       " -0.15405762,\n",
       " 1.3380917,\n",
       " 0.5243928,\n",
       " -1.0667969,\n",
       " -2.9179783,\n",
       " -0.19652648,\n",
       " 0.30864954,\n",
       " -0.3132557,\n",
       " 8.605688,\n",
       " 1.0636691,\n",
       " 0.020106554,\n",
       " 0.22456928,\n",
       " -0.9699101,\n",
       " 0.23929311,\n",
       " -1.5079457,\n",
       " -0.057182014,\n",
       " -0.14765234,\n",
       " 0.39279518,\n",
       " -0.04315722,\n",
       " 0.4428923,\n",
       " -0.28082392,\n",
       " -0.44303402,\n",
       " -0.7613438,\n",
       " 1.3988117,\n",
       " -21.543982,\n",
       " -0.0034104188,\n",
       " 1.2178365,\n",
       " 0.44976082,\n",
       " -0.047833968,\n",
       " 6.3664575,\n",
       " -0.6764469,\n",
       " 1.639836,\n",
       " 1.4463862,\n",
       " -0.63768023,\n",
       " -0.6828733,\n",
       " 0.4498006,\n",
       " 1.0363843,\n",
       " 0.8002835,\n",
       " -1.7560105,\n",
       " 0.25001433,\n",
       " 1.5401484,\n",
       " 0.46944737,\n",
       " 0.41955724,\n",
       " 0.18993698,\n",
       " 2.3571568,\n",
       " -0.19594245,\n",
       " -1.5669775,\n",
       " 0.36277854,\n",
       " 0.717228,\n",
       " 0.88069516,\n",
       " 0.28529152,\n",
       " 0.32926175,\n",
       " 0.91827506,\n",
       " 1.2460562,\n",
       " 0.33262917,\n",
       " -0.5641493,\n",
       " 0.6515973,\n",
       " 0.7501598,\n",
       " -0.33029148,\n",
       " 0.91694695,\n",
       " -1.4307051,\n",
       " 0.08343845,\n",
       " 0.49363783,\n",
       " 1.2024482,\n",
       " 0.17054836,\n",
       " -0.7837439,\n",
       " -0.37507653,\n",
       " 0.5875278,\n",
       " -0.14418578,\n",
       " 1.056013,\n",
       " 0.53745645,\n",
       " 1.7686018,\n",
       " 0.7270557,\n",
       " -0.5048521,\n",
       " -0.86043024,\n",
       " -0.79183483,\n",
       " -0.73740524,\n",
       " -0.040603787,\n",
       " -3.3082702,\n",
       " 0.08414843,\n",
       " -0.45483854,\n",
       " 0.16837786,\n",
       " 0.22724414,\n",
       " -0.5038323,\n",
       " 0.25243703,\n",
       " 0.80735666,\n",
       " -0.19548184,\n",
       " -0.56629723,\n",
       " 0.102695405,\n",
       " -0.06263685,\n",
       " -0.05026877,\n",
       " 0.4670701,\n",
       " -0.33387956,\n",
       " -0.35930955,\n",
       " -0.99523,\n",
       " -2.9137259,\n",
       " 0.67089176,\n",
       " 0.48770478,\n",
       " -0.69801146,\n",
       " -1.0666603,\n",
       " 1.4408994,\n",
       " -0.32670584,\n",
       " 0.15519397,\n",
       " 1.6887494,\n",
       " 1.6621456,\n",
       " 0.34152862,\n",
       " 1.9883639,\n",
       " 0.120113306,\n",
       " -0.16325715,\n",
       " -0.13402772,\n",
       " -0.21782736,\n",
       " -0.5371035,\n",
       " 0.11591611,\n",
       " -0.32854533,\n",
       " -0.45300904,\n",
       " 1.4287888,\n",
       " -0.7345008,\n",
       " 0.00944527,\n",
       " 0.86612964,\n",
       " -0.15668163,\n",
       " -0.20169549,\n",
       " 0.44310865,\n",
       " 0.3428929,\n",
       " 0.9591852,\n",
       " 1.1116983,\n",
       " 0.3302237,\n",
       " 0.03750342,\n",
       " -1.467264,\n",
       " 0.0121616125,\n",
       " -0.307829,\n",
       " -0.051745694,\n",
       " 0.05377623,\n",
       " 0.9779546,\n",
       " -0.5065714,\n",
       " 1.3777832,\n",
       " -0.80949885,\n",
       " -0.5159729,\n",
       " 0.39712802,\n",
       " -0.060945917,\n",
       " -0.14235404,\n",
       " 0.58584386,\n",
       " 0.14191921,\n",
       " -0.1025664,\n",
       " 0.55045074,\n",
       " 0.16506892,\n",
       " 0.16634224,\n",
       " -0.7791479,\n",
       " -0.75573367,\n",
       " -0.070905946,\n",
       " 1.3131174,\n",
       " 0.3340204,\n",
       " 0.107369356,\n",
       " -0.29361555,\n",
       " 0.18282415,\n",
       " 0.19399117,\n",
       " -0.5726523,\n",
       " -0.45350075,\n",
       " 0.035845608,\n",
       " 0.7896618,\n",
       " -1.1679267,\n",
       " 0.078192115,\n",
       " -2.3919265,\n",
       " -1.1093047,\n",
       " 0.8349428,\n",
       " -6.9386597,\n",
       " 0.26205596,\n",
       " -3.5945446,\n",
       " -0.2846368,\n",
       " -0.64422077,\n",
       " -0.029193303,\n",
       " -1.6018225,\n",
       " -1.754805,\n",
       " 0.3129291,\n",
       " -0.13199304,\n",
       " 1.2318107,\n",
       " -0.8230698,\n",
       " 0.41366282,\n",
       " -0.047982037,\n",
       " 0.48985872,\n",
       " -0.8419557,\n",
       " -0.33500263,\n",
       " 0.43965364,\n",
       " -1.0561548,\n",
       " -0.11588633,\n",
       " -0.41994393,\n",
       " -0.068802595,\n",
       " 0.88074714,\n",
       " -1.2466539,\n",
       " 0.693279,\n",
       " -0.84543234,\n",
       " 0.73158616,\n",
       " 0.26363257,\n",
       " -2.357155,\n",
       " -0.33065143,\n",
       " 2.77619,\n",
       " 0.13154285,\n",
       " 0.5923613,\n",
       " 0.44752678,\n",
       " 0.49759594,\n",
       " -0.9118061,\n",
       " -0.20744085,\n",
       " 0.49440932,\n",
       " -0.14360492,\n",
       " 0.32358196,\n",
       " -0.21349998,\n",
       " 0.40069208,\n",
       " 0.20084293,\n",
       " -0.27031603,\n",
       " -0.40924755,\n",
       " 0.46827802,\n",
       " 0.026890283,\n",
       " -1.6352383,\n",
       " 0.646492,\n",
       " 0.34494212,\n",
       " -0.10135945,\n",
       " 0.45444027,\n",
       " 0.63891023,\n",
       " 0.036718816,\n",
       " 0.03827999,\n",
       " -0.31243482,\n",
       " -1.0566672,\n",
       " -0.75458455,\n",
       " 0.21275848,\n",
       " 0.9416687,\n",
       " -0.6096168,\n",
       " -0.5781266,\n",
       " 0.032821473,\n",
       " 0.71778387,\n",
       " -1.0018,\n",
       " -0.3778375,\n",
       " 0.38681737,\n",
       " -0.36937824,\n",
       " 0.053459167,\n",
       " 1.280381,\n",
       " -0.9020954,\n",
       " -0.83064985,\n",
       " -0.8090032,\n",
       " 0.65206105,\n",
       " -0.6750609,\n",
       " -0.26079562,\n",
       " 1.3614818,\n",
       " -0.6403015,\n",
       " 0.6356738,\n",
       " 0.9381694,\n",
       " 0.101171084,\n",
       " 0.27531978,\n",
       " -0.17382038,\n",
       " -0.08793643,\n",
       " 0.7384657,\n",
       " -1.7878513,\n",
       " -1.3190184,\n",
       " -0.47883454,\n",
       " 0.3399143,\n",
       " -0.5878443,\n",
       " 0.3885137,\n",
       " 0.15082067,\n",
       " 1.1409925,\n",
       " -0.28327122,\n",
       " -0.39181575,\n",
       " 0.4900849,\n",
       " 0.18959014,\n",
       " -0.15054914,\n",
       " -0.6637966,\n",
       " -0.80284977,\n",
       " -0.9055899,\n",
       " 0.9367452,\n",
       " 0.25067967,\n",
       " -1.1177087,\n",
       " 0.55710316,\n",
       " 0.015721867,\n",
       " -0.4968842,\n",
       " 0.23840225,\n",
       " 0.10345101,\n",
       " -2.373396,\n",
       " 0.3566991,\n",
       " 0.33378354,\n",
       " 1.3898481,\n",
       " 2.4824383,\n",
       " 0.4775689,\n",
       " 0.3040758,\n",
       " 0.07354857,\n",
       " -0.22697906,\n",
       " 1.8032236,\n",
       " -0.45971727,\n",
       " -0.75585175,\n",
       " 0.9128704,\n",
       " -1.3250186,\n",
       " -2.9542472,\n",
       " 1.0829668,\n",
       " -0.08365691,\n",
       " 0.5582998,\n",
       " 0.59810907,\n",
       " 0.45013928,\n",
       " 0.5448641,\n",
       " 0.40085948,\n",
       " 1.4017922,\n",
       " 0.18111968,\n",
       " -0.17468315,\n",
       " -0.7715916,\n",
       " 0.5094579,\n",
       " -0.426091,\n",
       " -0.4178916,\n",
       " -0.98838234,\n",
       " 0.8719909,\n",
       " 0.9448802,\n",
       " -0.0008047819,\n",
       " -1.2993846,\n",
       " 0.19800758,\n",
       " 4.6045136,\n",
       " 0.5277362,\n",
       " -0.8070703,\n",
       " 0.61181384,\n",
       " -1.7967697,\n",
       " 1.0931222,\n",
       " -0.27464637,\n",
       " 0.19418593,\n",
       " 0.5434436,\n",
       " -0.173033,\n",
       " 0.14691475,\n",
       " -0.33610058,\n",
       " -1.1018928,\n",
       " -0.4981444,\n",
       " -0.8637643,\n",
       " 0.2729112,\n",
       " 0.2679911,\n",
       " 0.06949782,\n",
       " -0.8150068,\n",
       " -0.32320422,\n",
       " -0.38833097,\n",
       " 0.0033636491,\n",
       " 0.46870124,\n",
       " 0.3368646,\n",
       " 0.7153315,\n",
       " 0.46405554,\n",
       " -0.0014683405,\n",
       " 1.1592045,\n",
       " 0.711328,\n",
       " 1.9646279,\n",
       " -0.12313145,\n",
       " -0.23419811,\n",
       " 0.08771566,\n",
       " 1.1098427,\n",
       " 0.6718221,\n",
       " -0.5251978,\n",
       " -0.007656654,\n",
       " -0.044891953,\n",
       " 0.27571383,\n",
       " -0.13084131,\n",
       " 1.3389457,\n",
       " -0.65251213,\n",
       " 0.81192946,\n",
       " 0.52455163,\n",
       " 0.40154827,\n",
       " -1.1744522,\n",
       " 0.026302496,\n",
       " -0.10677093,\n",
       " -0.25734916,\n",
       " 0.5658446,\n",
       " -0.19453757,\n",
       " 0.19381213,\n",
       " -1.696061,\n",
       " -0.33068487,\n",
       " -0.12393639,\n",
       " 0.6356275,\n",
       " -0.43317997,\n",
       " 1.0020386,\n",
       " 0.5317011,\n",
       " 0.40394548,\n",
       " -0.0895196,\n",
       " 0.47027746,\n",
       " 0.54819274,\n",
       " 2.520304,\n",
       " -0.64946014,\n",
       " -0.5379454,\n",
       " 0.33908454,\n",
       " -1.3233413,\n",
       " 0.09783844,\n",
       " 0.90887994,\n",
       " 0.020632148,\n",
       " 1.6387577,\n",
       " 0.17490043,\n",
       " 0.40285647,\n",
       " 0.41191748,\n",
       " -0.17385042,\n",
       " -0.67737675,\n",
       " -0.89270025,\n",
       " -0.65658706,\n",
       " -0.305436,\n",
       " -0.27236712,\n",
       " 0.046218533,\n",
       " -0.57093805,\n",
       " 0.93661946,\n",
       " 0.383773,\n",
       " 0.6928124,\n",
       " 0.0066023073,\n",
       " 0.8322707,\n",
       " -1.1515735,\n",
       " 0.17131197,\n",
       " 0.30468413,\n",
       " -1.1937393,\n",
       " -8.718999,\n",
       " 0.3723507,\n",
       " 0.19979054,\n",
       " -0.4307975,\n",
       " 0.9851354,\n",
       " 1.0188742,\n",
       " -0.39103976,\n",
       " 1.2015333,\n",
       " 0.19816668,\n",
       " 0.655044,\n",
       " -0.974493,\n",
       " 0.002865553,\n",
       " -1.334712,\n",
       " 0.3551003,\n",
       " 1.6937194,\n",
       " 1.1302453,\n",
       " 0.29345587,\n",
       " 0.034811705,\n",
       " -1.1146299,\n",
       " -0.6213017,\n",
       " 0.28995156,\n",
       " -0.5522759,\n",
       " 0.40083995,\n",
       " 1.516174,\n",
       " 1.8303953,\n",
       " -0.15544288,\n",
       " 0.66084164,\n",
       " 0.57568663,\n",
       " -0.4333841,\n",
       " -0.29780152,\n",
       " 0.0988546,\n",
       " 0.26546538,\n",
       " 0.7380224,\n",
       " 0.2126698,\n",
       " 0.23846494,\n",
       " 1.5642675,\n",
       " -0.96367484,\n",
       " -0.007320404,\n",
       " -1.5869299,\n",
       " -0.0673484,\n",
       " 0.7695422,\n",
       " 3.5679562,\n",
       " 0.58196026,\n",
       " 1.4176351,\n",
       " 0.7308667,\n",
       " 2.251701,\n",
       " 0.15138231,\n",
       " 0.29762945,\n",
       " -0.15682752,\n",
       " -0.5014812,\n",
       " -0.80235726,\n",
       " 0.52775496,\n",
       " -0.38208368,\n",
       " 0.068376325,\n",
       " -0.32275453,\n",
       " -0.021930115,\n",
       " 0.3731198,\n",
       " -0.5124612,\n",
       " 0.4154412,\n",
       " 1.9566551,\n",
       " 0.8770673,\n",
       " -0.058777828,\n",
       " 1.602923,\n",
       " -0.023783186,\n",
       " 0.902693,\n",
       " -1.051661,\n",
       " -1.8212029,\n",
       " -0.8162468,\n",
       " -0.83023566,\n",
       " 0.5751446,\n",
       " 0.04847655,\n",
       " -0.5265926,\n",
       " -0.0653145,\n",
       " -1.5101732,\n",
       " 0.13341333,\n",
       " 0.31643105,\n",
       " 0.9568043,\n",
       " -0.9600772,\n",
       " -0.14555885,\n",
       " 0.6506434,\n",
       " 0.2310496,\n",
       " -0.81457406,\n",
       " 0.66837114,\n",
       " 0.88111633,\n",
       " 0.8940839,\n",
       " -0.8104102,\n",
       " -0.04328488,\n",
       " 0.1891339,\n",
       " 0.35458997,\n",
       " 1.0289081,\n",
       " -0.9382636,\n",
       " -0.31578314,\n",
       " 0.74704725,\n",
       " 0.026916549,\n",
       " -0.14001748,\n",
       " 0.015957892,\n",
       " 0.54693526,\n",
       " 0.19484524,\n",
       " 3.0710232,\n",
       " 0.3262469,\n",
       " 0.47872877,\n",
       " 1.1062014,\n",
       " 0.54612,\n",
       " -0.031809967,\n",
       " 0.7832551,\n",
       " -0.10187032,\n",
       " 0.7434314,\n",
       " -0.27024952,\n",
       " 0.028192004,\n",
       " -0.61376625,\n",
       " -0.56677485,\n",
       " -0.091262855,\n",
       " -0.44088867,\n",
       " -0.23842688,\n",
       " 0.09852787,\n",
       " -0.27939072,\n",
       " 0.46261203,\n",
       " -1.1797632,\n",
       " -0.6821294,\n",
       " -0.039720166,\n",
       " -0.40990368,\n",
       " 0.20181273,\n",
       " -0.1319058,\n",
       " -0.75408906,\n",
       " 0.80952567,\n",
       " -0.8338423,\n",
       " 0.17875516,\n",
       " -0.8988247,\n",
       " 0.32917237,\n",
       " 0.23529102,\n",
       " -1.0536097,\n",
       " 0.11927948,\n",
       " 1.22987,\n",
       " 0.106076,\n",
       " 0.21427941,\n",
       " 72.67722,\n",
       " -0.8079479,\n",
       " 0.0764362,\n",
       " 0.13263424,\n",
       " -0.32817873,\n",
       " -0.22945201,\n",
       " -1.1641288,\n",
       " -0.53433543,\n",
       " 0.71356153,\n",
       " -0.097552024,\n",
       " -0.25245157,\n",
       " 0.774072,\n",
       " 0.8033653,\n",
       " 1.7597594,\n",
       " 0.23818403,\n",
       " -0.9896441,\n",
       " -0.070730366,\n",
       " 0.58866644,\n",
       " -0.60863787,\n",
       " 0.20174904,\n",
       " -0.25146964,\n",
       " 1.2895232,\n",
       " -0.33319548,\n",
       " 1.554664,\n",
       " 0.48659804,\n",
       " 0.55896115,\n",
       " -2.0341492,\n",
       " 0.5338302,\n",
       " -0.12289099,\n",
       " 0.47764158,\n",
       " 2.0984764,\n",
       " 0.078972705,\n",
       " 0.62612677,\n",
       " 0.7896351,\n",
       " 0.114046335,\n",
       " 0.7675257,\n",
       " 0.73028594,\n",
       " 0.05133645,\n",
       " 0.4146562,\n",
       " 1.4393059,\n",
       " -0.7042162,\n",
       " 0.10922936,\n",
       " -0.60124534,\n",
       " -1.0396309,\n",
       " -0.64512974,\n",
       " -0.0885336,\n",
       " -0.75185853,\n",
       " 0.036456484,\n",
       " 0.28957316,\n",
       " -0.05066224,\n",
       " -0.31618658,\n",
       " -0.1266976,\n",
       " 0.0428313,\n",
       " 0.012320206,\n",
       " -0.32250556,\n",
       " -2.027818,\n",
       " -1.3025885,\n",
       " -0.11222123,\n",
       " -0.06638181,\n",
       " 0.27528837,\n",
       " 0.96351165,\n",
       " 0.63172257,\n",
       " -0.29950455,\n",
       " 0.23901011,\n",
       " -0.34854177,\n",
       " -0.37406853,\n",
       " -1.1257149,\n",
       " 0.9174548,\n",
       " -1.3773584,\n",
       " -0.14581098,\n",
       " -0.4977325,\n",
       " -0.5269353,\n",
       " 0.51091456,\n",
       " -0.144262,\n",
       " 1.1081501,\n",
       " -1.4189919,\n",
       " -1.1130284,\n",
       " -0.2733155,\n",
       " -0.37901378,\n",
       " -0.0661029,\n",
       " 0.16477646,\n",
       " 1.0043197,\n",
       " 0.41133404,\n",
       " -0.017965654,\n",
       " 0.6159161,\n",
       " 0.92552894,\n",
       " 1.4946728,\n",
       " 0.55237895,\n",
       " -0.742593,\n",
       " 0.8008654,\n",
       " 0.08172774,\n",
       " 0.08629811,\n",
       " 0.52898574,\n",
       " -0.6259225,\n",
       " -0.20304793,\n",
       " -0.8567304,\n",
       " 0.39127362,\n",
       " 0.03849739,\n",
       " 1.0249766,\n",
       " -0.26990804,\n",
       " 0.79094,\n",
       " 0.070748486,\n",
       " -0.32684857,\n",
       " 0.7121331,\n",
       " -0.36301675,\n",
       " 1.6915888,\n",
       " 0.7038867,\n",
       " 0.16632533,\n",
       " 0.16531797,\n",
       " -0.29207772,\n",
       " -0.10354974,\n",
       " 0.47648644,\n",
       " 0.48251915,\n",
       " -0.6204117,\n",
       " -2.0025795,\n",
       " 0.03635953,\n",
       " 0.042965572,\n",
       " 0.9473117,\n",
       " -0.17575611,\n",
       " -1.1084747,\n",
       " -0.12072202,\n",
       " 0.4597126,\n",
       " 0.76259995,\n",
       " -0.10731977,\n",
       " -0.39929262,\n",
       " 0.42517003,\n",
       " -0.030999118,\n",
       " -0.21861692,\n",
       " -0.41830292,\n",
       " 0.49267337,\n",
       " 0.78094107,\n",
       " -0.58078307,\n",
       " 1.3333393,\n",
       " -0.092424355,\n",
       " -0.13226382,\n",
       " -0.20317589,\n",
       " -1.0022641,\n",
       " 1.1715716,\n",
       " -1.2641128,\n",
       " -0.02300332,\n",
       " 1.3988141,\n",
       " -0.3664005,\n",
       " -0.17321111,\n",
       " 0.34738073,\n",
       " 0.39309332,\n",
       " 1.0960358,\n",
       " 2.9412115,\n",
       " 1.1756988,\n",
       " 0.31075555,\n",
       " 0.9076808,\n",
       " -0.21898317,\n",
       " -0.59895724,\n",
       " -0.27008483,\n",
       " -1.3777243,\n",
       " 0.05784905,\n",
       " 0.12291821,\n",
       " 0.46145844,\n",
       " 1.823335,\n",
       " 0.11191424,\n",
       " -0.52968425,\n",
       " -0.7206723,\n",
       " -1.6888376,\n",
       " -0.32243466,\n",
       " -0.7818355,\n",
       " -1.3624624,\n",
       " -1.4171911,\n",
       " -0.07241428,\n",
       " 0.49072435,\n",
       " -0.46569327,\n",
       " 0.11690775,\n",
       " -0.49700525,\n",
       " -0.08970753,\n",
       " -0.13480845,\n",
       " -0.5641587,\n",
       " -0.22495861,\n",
       " -0.01057003,\n",
       " 2.9079115,\n",
       " 0.26201132,\n",
       " 0.42766714,\n",
       " 0.3467598]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_embedding(token_vecs_sum[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n"
     ]
    }
   ],
   "source": [
    "token_indices = find_token_index(tokenized_text)\n",
    "embeddings = []\n",
    "for token_index in token_indices:\n",
    "    start_index = token_index[0]\n",
    "    end_index = token_index[-1] + 1\n",
    "    if len(token_index) != 1:\n",
    "        embeddings.append(get_mean_embedding(token_vecs_sum[start_index:end_index]))\n",
    "    else:\n",
    "        print(token_index)\n",
    "        embeddings.append(token_vecs_sum[token_index[0]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35, 36], [37], [38], [39], [40], [41], [42]]\n"
     ]
    }
   ],
   "source": [
    "print(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621639678875605"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(embeddings[0:3],axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621639678875605"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a sannity check\n",
    "v = 0\n",
    "for i in range(0,3):\n",
    "    v += embeddings[i][0]\n",
    "v/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
