{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scorer import *\n",
    "import _pickle as cPickle\n",
    "\n",
    "for pack in os.listdir(\"src\"):\n",
    "    sys.path.append(os.path.join(\"src\", pack))\n",
    "\n",
    "sys.path.append(\"/src/shared/\")\n",
    "\n",
    "out_dir = \"model/\"\n",
    "config_path = \"train_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "logging.basicConfig(filename=os.path.join(out_dir, \"train_log.txt\"),\n",
    "                    level=logging.DEBUG, filemode='w')\n",
    "\n",
    "# Load json config file\n",
    "with open(config_path, 'r') as js_file:\n",
    "    config_dict = json.load(js_file)\n",
    "\n",
    "with open(os.path.join(out_dir,'train_config.json'), \"w\") as js_file:\n",
    "    json.dump(config_dict, js_file, indent=4, sort_keys=True)\n",
    "\n",
    "random.seed(config_dict[\"random_seed\"])\n",
    "np.random.seed(config_dict[\"random_seed\"])\n",
    "\n",
    "if config_dict[\"gpu_num\"] != -1:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config_dict[\"gpu_num\"])\n",
    "    use_cuda = True\n",
    "else:\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CUDA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "use_cuda = use_cuda and torch.cuda.is_available()\n",
    "\n",
    "import torch.nn as nn\n",
    "from classes import *\n",
    "from eval_utils import *\n",
    "from model_utils import *\n",
    "from model_factory import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "torch.manual_seed(config_dict[\"seed\"])\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(config_dict[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print('Training with CUDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, dev_set, clusterer):\n",
    "    '''\n",
    "    Initializes models, optimizers and loss functions, then, it runs the training procedure that\n",
    "    alternates between entity and event training and clustering on the train set.\n",
    "    After each epoch, it runs the inference procedure on the dev set and calculates\n",
    "    the B-cubed measure and use it to tune the model and its hyper-parameters.\n",
    "    Saves the entity and event models that achieved the best B-cubed scores on the dev set.\n",
    "    :param train_set: a Corpus object, representing the train set.\n",
    "    :param dev_set: a Corpus object, representing the dev set.\n",
    "    '''\n",
    "    device = torch.device(\"cuda:0\" if args.use_cuda else \"cpu\")\n",
    "\n",
    "    doc_to_entity_mentions = load_entity_wd_clusters(config_dict)  # loads predicted WD entity coref chains from external tool\n",
    "\n",
    "    print('Create new models...')\n",
    "    logging.info('Create new models...')\n",
    "    factory_load_embeddings(config_dict)  # loading pre-trained embeddings before creating new models\n",
    "    cd_event_model = create_model(config_dict)\n",
    "    cd_entity_model = create_model(config_dict)\n",
    "\n",
    "    cd_event_model = cd_event_model.to(device)\n",
    "    cd_entity_model = cd_entity_model.to(device)\n",
    "\n",
    "    cd_event_optimizer = create_optimizer(config_dict, cd_event_model)\n",
    "    cd_entity_optimizer = create_optimizer(config_dict, cd_entity_model)\n",
    "\n",
    "    cd_event_loss = create_loss(config_dict)\n",
    "    cd_entity_loss = create_loss(config_dict)\n",
    "\n",
    "    topics = train_set.topics  # Use the gold sub-topics\n",
    "\n",
    "    topics_num = len(topics.keys())\n",
    "    event_best_dev_f1 = 0\n",
    "    entity_best_dev_f1 = 0\n",
    "    best_event_epoch = 0\n",
    "    best_entity_epoch = 0\n",
    "\n",
    "    patient_counter = 0\n",
    "\n",
    "    orig_event_th = config_dict[\"event_merge_threshold\"]\n",
    "    orig_entity_th = config_dict[\"entity_merge_threshold\"]\n",
    "    for epoch in range(1, config_dict[\"epochs\"]):\n",
    "        logging.info('Epoch {}:'.format(str(epoch)))\n",
    "        print('Epoch {}:'.format(str(epoch)))\n",
    "        topics_counter = 0\n",
    "        topics_keys = list(topics.keys())\n",
    "        random.shuffle(topics_keys)\n",
    "        for topic_id in topics_keys:\n",
    "            topics_counter += 1\n",
    "            topic = topics[topic_id]\n",
    "\n",
    "            logging.info('=========================================================================')\n",
    "            logging.info('Topic {}:'.format(topic_id))\n",
    "            print('Topic {}:'.format(topic_id))\n",
    "\n",
    "            # init event and entity clusters\n",
    "            event_mentions, entity_mentions = topic_to_mention_list(topic, is_gold=True)\n",
    "\n",
    "            if config_dict[\"train_init_wd_entity_with_gold\"]:\n",
    "                # initialize entity clusters with gold within document entity coreference chains\n",
    "                wd_entity_clusters = create_gold_wd_clusters_organized_by_doc(entity_mentions, is_event=False)\n",
    "            else:\n",
    "                # initialize entity clusters with within document entity coreference system output\n",
    "                wd_entity_clusters = init_entity_wd_clusters(entity_mentions, doc_to_entity_mentions)\n",
    "\n",
    "            entity_clusters = []\n",
    "            for doc_id, clusters in wd_entity_clusters.items():\n",
    "                entity_clusters.extend(clusters)\n",
    "\n",
    "            event_clusters = init_cd(event_mentions, is_event=True)\n",
    "\n",
    "            # initialize cluster representation\n",
    "            update_lexical_vectors(entity_clusters, cd_entity_model, device,\n",
    "                                   is_event=False, requires_grad=False)\n",
    "            update_lexical_vectors(event_clusters, cd_event_model, device,\n",
    "                                   is_event=True, requires_grad=False)\n",
    "\n",
    "            entity_th = config_dict[\"entity_merge_threshold\"]\n",
    "            event_th = config_dict[\"event_merge_threshold\"]\n",
    "\n",
    "            for i in range(1,config_dict[\"merge_iters\"]+1):\n",
    "                print('Iteration number {}'.format(i))\n",
    "                logging.info('Iteration number {}'.format(i))\n",
    "\n",
    "\n",
    "                # Entities\n",
    "                print('Train entity model and merge entity clusters...')\n",
    "                logging.info('Train entity model and merge entity clusters...')\n",
    "                train_and_merge(clusterer, clusters=entity_clusters, other_clusters=event_clusters,\n",
    "                                model=cd_entity_model, optimizer=cd_entity_optimizer,\n",
    "                                loss=cd_entity_loss,device=device,topic=topic,is_event=False,epoch=epoch,\n",
    "                                topics_counter=topics_counter, topics_num=topics_num,\n",
    "                                threshold=entity_th)\n",
    "                # Events\n",
    "                print('Train event model and merge event clusters...')\n",
    "                logging.info('Train event model and merge event clusters...')\n",
    "                train_and_merge(clusterer, clusters=event_clusters, other_clusters=entity_clusters,\n",
    "                                model=cd_event_model, optimizer=cd_event_optimizer,\n",
    "                                loss=cd_event_loss,device=device,topic=topic,is_event=True,epoch=epoch,\n",
    "                                topics_counter=topics_counter, topics_num=topics_num,\n",
    "                                threshold=event_th)\n",
    "\n",
    "        print('Testing models on dev set...')\n",
    "        logging.info('Testing models on dev set...')\n",
    "\n",
    "        threshold_list = config_dict[\"dev_th_range\"]\n",
    "        improved = False\n",
    "        best_event_f1_for_th = 0\n",
    "        best_entity_f1_for_th = 0\n",
    "\n",
    "        if event_best_dev_f1 > 0:\n",
    "            best_saved_cd_event_model = load_check_point(os.path.join(args.out_dir,\n",
    "                                                                      'cd_event_best_model'))\n",
    "            best_saved_cd_event_model.to(device)\n",
    "        else:\n",
    "            best_saved_cd_event_model = cd_event_model\n",
    "\n",
    "        if entity_best_dev_f1 > 0:\n",
    "            best_saved_cd_entity_model = load_check_point(os.path.join(args.out_dir,\n",
    "                                                                       'cd_entity_best_model'))\n",
    "            best_saved_cd_entity_model.to(device)\n",
    "        else:\n",
    "            best_saved_cd_entity_model = cd_entity_model\n",
    "\n",
    "        for event_threshold in threshold_list:\n",
    "            for entity_threshold in threshold_list:\n",
    "                config_dict[\"event_merge_threshold\"] = event_threshold\n",
    "                config_dict[\"entity_merge_threshold\"] = entity_threshold\n",
    "                print('Testing models on dev set with threshold={}'.format((event_threshold,entity_threshold)))\n",
    "                logging.info('Testing models on dev set with threshold={}'.format((event_threshold,entity_threshold)))\n",
    "\n",
    "                # test event coref on dev\n",
    "                event_f1, _ = test_models(dev_set, cd_event_model, best_saved_cd_entity_model, device,\n",
    "                                                  config_dict, write_clusters=False, out_dir=args.out_dir,\n",
    "                                                  doc_to_entity_mentions=doc_to_entity_mentions, analyze_scores=False)\n",
    "\n",
    "                # test entity coref on dev\n",
    "                _, entity_f1 = test_models(dev_set, best_saved_cd_event_model, cd_entity_model, device,\n",
    "                                                  config_dict, write_clusters=False, out_dir=args.out_dir,\n",
    "                                                  doc_to_entity_mentions=doc_to_entity_mentions, analyze_scores=False)\n",
    "\n",
    "                if event_f1 > best_event_f1_for_th:\n",
    "                    best_event_f1_for_th = event_f1\n",
    "                    best_event_th = (event_threshold,entity_threshold)\n",
    "\n",
    "                if entity_f1 > best_entity_f1_for_th:\n",
    "                    best_entity_f1_for_th = entity_f1\n",
    "                    best_entity_th = (event_threshold,entity_threshold)\n",
    "\n",
    "        event_f1 = best_event_f1_for_th\n",
    "        entity_f1 = best_entity_f1_for_th\n",
    "        save_epoch_f1(event_f1, entity_f1, epoch, best_event_th, best_entity_th)\n",
    "\n",
    "        config_dict[\"event_merge_threshold\"] = orig_event_th\n",
    "        config_dict[\"entity_merge_threshold\"] = orig_entity_th\n",
    "\n",
    "        if event_f1 > event_best_dev_f1:\n",
    "            event_best_dev_f1 = event_f1\n",
    "            best_event_epoch = epoch\n",
    "            save_check_point(cd_event_model, os.path.join(args.out_dir, 'cd_event_best_model'))\n",
    "            improved = True\n",
    "            patient_counter = 0\n",
    "        if entity_f1 > entity_best_dev_f1:\n",
    "            entity_best_dev_f1 = entity_f1\n",
    "            best_entity_epoch = epoch\n",
    "            save_check_point(cd_entity_model, os.path.join(args.out_dir, 'cd_entity_best_model'))\n",
    "            improved = True\n",
    "            patient_counter = 0\n",
    "\n",
    "        if not improved:\n",
    "            patient_counter += 1\n",
    "\n",
    "        save_training_checkpoint(epoch, cd_event_model, cd_event_optimizer, event_best_dev_f1,\n",
    "                                 filename=os.path.join(args.out_dir, 'cd_event_model_state'))\n",
    "        save_training_checkpoint(epoch, cd_entity_model, cd_entity_optimizer, entity_best_dev_f1,\n",
    "                                 filename=os.path.join(args.out_dir, 'cd_entity_model_state'))\n",
    "\n",
    "        if patient_counter >= config_dict[\"patient\"]:\n",
    "            logging.info('Early Stopping!')\n",
    "            print('Early Stopping!')\n",
    "            save_summary(event_best_dev_f1, entity_best_dev_f1, best_event_epoch, best_entity_epoch, epoch)\n",
    "            break\n",
    "\n",
    "\n",
    "def train_and_merge(clusterer, clusters, other_clusters, model, optimizer,\n",
    "                    loss, device, topic ,is_event, epoch,\n",
    "                    topics_counter, topics_num, threshold):\n",
    "    '''\n",
    "    This function trains event/entity and then uses agglomerative clustering algorithm that\n",
    "    merges event/entity clusters\n",
    "    :param clusters: current event/entity clusters\n",
    "    :param other_clusters: should be the event current clusters if clusters = entity clusters\n",
    "    and vice versa.\n",
    "    :param model: event/entity model (according to clusters parameter)\n",
    "    :param optimizer: event/entity optimizer (according to clusters parameter)\n",
    "    :param loss: event/entity loss (according to clusters parameter)\n",
    "    :param device: gpu/cpu Pytorch device\n",
    "    :param topic: Topic object represents the current topic\n",
    "    :param is_event: whether to currently handle event mentions or entity mentions\n",
    "    :param epoch: current epoch number\n",
    "    :param topics_counter: the number of current topic\n",
    "    :param topics_num: total number of topics\n",
    "    :param threshold: merging threshold\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Update arguments/predicates vectors according to the other clusters state\n",
    "    update_args_feature_vectors(clusters, other_clusters, model, device, is_event)\n",
    "\n",
    "    cluster_pairs,test_cluster_pairs \\\n",
    "        = generate_cluster_pairs(clusters, is_train=True)\n",
    "\n",
    "    # Train pairwise event/entity coreference scorer\n",
    "    train(cluster_pairs, model, optimizer, loss,\n",
    "          device, topic.docs, epoch, topics_counter, topics_num, config_dict, is_event,\n",
    "          other_clusters)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        update_lexical_vectors(clusters, model, device ,is_event, requires_grad=False)\n",
    "\n",
    "        event_mentions, entity_mentions = topic_to_mention_list(topic, is_gold=True)\n",
    "\n",
    "        # Update span representations after training\n",
    "        create_mention_span_representations(event_mentions, model, device, topic.docs, is_event=True,\n",
    "                                            requires_grad=False)\n",
    "        create_mention_span_representations(entity_mentions, model, device, topic.docs, is_event=False,\n",
    "                                            requires_grad=False)\n",
    "\n",
    "        cluster_pairs = test_cluster_pairs\n",
    "\n",
    "        # Merge clusters till reaching the threshold\n",
    "        merge(clusterer, clusters, cluster_pairs, other_clusters, model, device, topic.docs, epoch,\n",
    "              topics_counter, topics_num, threshold, is_event,\n",
    "              config_dict[\"use_args_feats\"], config_dict[\"use_binary_feats\"])\n",
    "\n",
    "\n",
    "def save_epoch_f1(event_f1, entity_f1, epoch,  best_event_th, best_entity_th):\n",
    "    '''\n",
    "    Write to a text file B-cubed F1 measures of both event and entity clustering\n",
    "    according to the models' predictions on the dev set after each training epoch.\n",
    "    :param event_f1: B-cubed F1 measure for event coreference\n",
    "    :param entity_f1: B-cubed F1 measure for entity coreference\n",
    "    :param epoch: current epoch number\n",
    "    :param best_event_th: best found merging threshold for event coreference\n",
    "    :param best_entity_th: best found merging threshold for event coreference\n",
    "    '''\n",
    "    f = open(os.path.join(args.out_dir,'epochs_scores.txt'),'a')\n",
    "    f.write('Epoch {} -  Event F1: {:.3f} with th = {}  Entity F1: {:.3f} with th = {}  \\n'.format(epoch,event_f1,best_event_th, entity_f1, best_entity_th))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def save_summary(best_event_score,best_entity_score, best_event_epoch,best_entity_epoch, total_epochs):\n",
    "    '''\n",
    "    Writes to a file a summary of the training (best scores, their epochs, and total number of\n",
    "    epochs)\n",
    "    :param best_event_score: best event coreference score on the dev set\n",
    "    :param best_entity_score: best entity coreference score on the dev set\n",
    "    :param best_event_epoch: the epoch of the best event coreference\n",
    "    :param best_entity_epoch: the epoch of the best entity coreference\n",
    "    :param total_epochs: total number of epochs\n",
    "    '''\n",
    "    f = open(os.path.join(args.out_dir, 'summary.txt'), 'w')\n",
    "    f.write('Best Event F1: {:.3f} epoch: {} \\n Best Entity F1: {:.3f} epoch: '\n",
    "            '{} \\n Training epochs: {}'.format(best_event_score,best_event_epoch,best_entity_score\n",
    "                                               ,best_entity_epoch, total_epochs))\n",
    "\n",
    "\n",
    "def save_training_checkpoint(epoch, model, optimizer, best_f1, filename):\n",
    "    '''\n",
    "    Saves model's checkpoint after each epoch\n",
    "    :param epoch: epoch number\n",
    "    :param model: the model to save\n",
    "    :param optimizer: Pytorch optimizer\n",
    "    :param best_f1: the best B-cubed F1 score so far\n",
    "    :param filename: the filename of the checkpoint file\n",
    "    '''\n",
    "    state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'best_f1': best_f1 }\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_training_checkpoint(model, optimizer, filename, device):\n",
    "    '''\n",
    "    Loads checkpoint from a file\n",
    "    :param model: an initialized model (CDCorefScorer)\n",
    "    :param optimizer: new Pytorch optimizer\n",
    "    :param filename: the checkpoint filename\n",
    "    :param device: gpu/cpu device\n",
    "    :return: model, optimizer, epoch, best_f1 loaded from the checkpoint.\n",
    "    '''\n",
    "    print(\"Loading checkpoint '{}'\".format(filename))\n",
    "    checkpoint = torch.load(filename)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(\"Loaded checkpoint '{}' (epoch {})\"\n",
    "                .format(filename, checkpoint['epoch']))\n",
    "\n",
    "    model = model.to(device)\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    return model, optimizer, start_epoch, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_dict[\"train_path\"], 'rb') as f:\n",
    "    train_set = cPickle.load(f)\n",
    "with open(config_dict[\"dev_path\"], 'rb') as f:\n",
    "    dev_set = cPickle.load(f)\n",
    "with open(\"data/feature_ELMO/test_data\", 'rb') as f:\n",
    "    test_set = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = train_set.topics  # Use the gold sub-topics\n",
    "topics_num = len(topics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<classes.Corpus at 0x7f5cf81e7e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new models...\n",
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "doc_to_entity_mentions = load_entity_wd_clusters(config_dict)  # loads predicted WD entity coref chains from external tool\n",
    "\n",
    "print('Create new models...')\n",
    "logging.info('Create new models...')\n",
    "factory_load_embeddings(config_dict)  # loading pre-trained embeddings before creating new models\n",
    "cd_event_model = create_model(config_dict)\n",
    "cd_entity_model = create_model(config_dict)\n",
    "\n",
    "cd_event_model = cd_event_model.to(device)\n",
    "cd_entity_model = cd_entity_model.to(device)\n",
    "\n",
    "cd_event_optimizer = create_optimizer(config_dict, cd_event_model)\n",
    "cd_entity_optimizer = create_optimizer(config_dict, cd_entity_model)\n",
    "\n",
    "cd_event_loss = create_loss(config_dict)\n",
    "cd_entity_loss = create_loss(config_dict)\n",
    "\n",
    "topics = train_set.topics  # Use the gold sub-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_counter = 0\n",
    "topics_keys = list(topics.keys())\n",
    "random.shuffle(topics_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29_ecbplus',\n",
       " '29_ecb',\n",
       " '26_ecb',\n",
       " '8_ecbplus',\n",
       " '30_ecbplus',\n",
       " '16_ecb',\n",
       " '33_ecbplus',\n",
       " '16_ecbplus',\n",
       " '3_ecb',\n",
       " '9_ecb',\n",
       " '19_ecbplus',\n",
       " '32_ecbplus',\n",
       " '10_ecbplus',\n",
       " '26_ecbplus',\n",
       " '14_ecbplus',\n",
       " '19_ecb',\n",
       " '14_ecb',\n",
       " '25_ecb',\n",
       " '22_ecbplus',\n",
       " '20_ecbplus',\n",
       " '1_ecbplus',\n",
       " '7_ecb',\n",
       " '6_ecbplus',\n",
       " '6_ecb',\n",
       " '28_ecb',\n",
       " '3_ecbplus',\n",
       " '22_ecb',\n",
       " '13_ecb',\n",
       " '32_ecb',\n",
       " '7_ecbplus',\n",
       " '28_ecbplus',\n",
       " '4_ecbplus',\n",
       " '13_ecbplus',\n",
       " '25_ecbplus',\n",
       " '20_ecb',\n",
       " '8_ecb',\n",
       " '11_ecb',\n",
       " '4_ecb',\n",
       " '10_ecb',\n",
       " '1_ecb',\n",
       " '30_ecb',\n",
       " '24_ecb',\n",
       " '31_ecb',\n",
       " '24_ecbplus',\n",
       " '27_ecbplus',\n",
       " '31_ecbplus',\n",
       " '33_ecb',\n",
       " '11_ecbplus',\n",
       " '9_ecbplus',\n",
       " '27_ecb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all mentions\n",
    "all_event_mentions = []\n",
    "all_entity_mentions = []\n",
    "for topic_id in topics_keys:\n",
    "    # init event and entity clusters\n",
    "    topic = topics[topic_id]\n",
    "    event_mentions, entity_mentions = topic_to_mention_list(topic, is_gold=True)\n",
    "    all_event_mentions.extend(event_mentions)\n",
    "    all_entity_mentions.extend(event_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_bow_arg_vec(mentions, model, device):\n",
    "    for event_mention in mentions:\n",
    "        event_mention.arg0_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "        event_mention.arg1_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "        event_mention.time_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "        event_mention.loc_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "\n",
    "def create_entity_bow_predicate_vec(mentions, model, device):\n",
    "\n",
    "    for entity_mention in mentions:\n",
    "        entity_mention.arg0_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "        entity_mention.arg1_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "        entity_mention.time_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "        entity_mention.loc_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim, requires_grad=False).to(device).view(1, -1)\n",
    "                \n",
    "                \n",
    "def create_event_bow_lexical_vec(event_mention, model, device, config_dict):\n",
    "\n",
    "    if config_dict[\"use_char_embeds\"]:\n",
    "        bow_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim,\n",
    "                              requires_grad=config_dict[\"requires_grad\"]).to(device).view(1, -1)\n",
    "    else:\n",
    "        bow_vec = torch.zeros(model.embedding_dim, requires_grad=config_dict[\"requires_grad\"]).to(device).view(1, -1)\n",
    "    \n",
    "    # creating lexical vector using the head word of each event mention in the cluster\n",
    "    head = event_mention.mention_head\n",
    "    head_tensor = find_word_embed(head, model, device)\n",
    "    if config_dict[\"use_char_embeds\"]:\n",
    "        char_tensor = get_char_embed(head, model, device)\n",
    "        if not config_dict[\"requires_grad\"]:\n",
    "            char_tensor = char_tensor.detach()\n",
    "        cat_tensor = torch.cat([head_tensor, char_tensor], 1)\n",
    "    else:\n",
    "        cat_tensor = head_tensor\n",
    "    bow_vec += cat_tensor\n",
    "\n",
    "    return bow_vec \n",
    "\n",
    "\n",
    "def create_entity_cluster_bow_lexical_vec(entity_mention, model, device, config_dict):\n",
    "    if config_dict[\"use_char_embeds\"]:\n",
    "        bow_vec = torch.zeros(model.embedding_dim + model.char_hidden_dim,\n",
    "                              requires_grad=config_dict[\"requires_grad\"]).to(device).view(1, -1)\n",
    "    else:\n",
    "        bow_vec = torch.zeros(model.embedding_dim,requires_grad=config_dict[\"requires_grad\"]).to(device).view(1, -1)\n",
    "\n",
    "    # creating lexical vector using each entity mention in the cluster\n",
    "    mention_bow = torch.zeros(model.embedding_dim,requires_grad=config_dict[\"requires_grad\"]).to(device).view(1, -1)\n",
    "    mention_embeds = [find_word_embed(token, model, device)\n",
    "                      for token in entity_mention.get_tokens()\n",
    "                      if not is_stop(token)]\n",
    "    if config_dict[\"use_char_embeds\"]:\n",
    "        char_embeds = get_char_embed(entity_mention.mention_str, model, device)\n",
    "\n",
    "    for word_tensor in mention_embeds:\n",
    "        mention_bow += word_tensor\n",
    "\n",
    "    mention_bow /= len(entity_mention.get_tokens())\n",
    "\n",
    "    if config_dict[\"use_char_embeds\"]:\n",
    "        if not config_dict[\"requires_grad\"]:\n",
    "            char_embeds = char_embeds.detach()\n",
    "\n",
    "        cat_tensor = torch.cat([mention_bow, char_embeds], 1)\n",
    "    else:\n",
    "        cat_tensor = mention_bow\n",
    "    bow_vec += cat_tensor\n",
    "\n",
    "    return bow_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lexical_vectors(mentions, model, device, config_dict, is_event):    \n",
    "    if is_event:\n",
    "        lex_vec = create_event_cluster_bow_lexical_vec(cluster, model, device, config_dict)\n",
    "    else:\n",
    "        lex_vec = create_entity_cluster_bow_lexical_vec(cluster, model, device, config_dict)\n",
    "\n",
    "    cluster.lex_vec = lex_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_args_feature_vectors(clusters, other_clusters, model, device, is_event)\n",
    "update_args_feature_vectors(clusters, other_clusters, model, device, is_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mention_to_input(mention, model, device, config_dict, is_event):\n",
    "\n",
    "    # create span representation\n",
    "    if config_dict[\"requires_grad\"]:\n",
    "        mention.span_rep = get_mention_span_rep(mention, device, model, config_dict=config_dict, is_event=is_event)\n",
    "    span_rep = mention.span_rep\n",
    "\n",
    "    if config_dict[\"use_args_feats\"]:\n",
    "        mention_tensor = torch.cat([span_rep, mention.arg0_vec, mention.arg1_vec,\n",
    "                                      mention.loc_vec, mention.time_vec], 1)\n",
    "\n",
    "    else:\n",
    "        mention_1_tensor = span_rep_1\n",
    "        mention_2_tensor = span_rep_2\n",
    "\n",
    "    if model.use_mult and model.use_diff:\n",
    "        mention_pair_tensor = torch.cat([mention_1_tensor, mention_2_tensor,\n",
    "                                         mention_1_tensor - mention_2_tensor,\n",
    "                                         mention_1_tensor * mention_2_tensor], 1)\n",
    "    elif model.use_mult:\n",
    "        mention_pair_tensor = torch.cat([mention_1_tensor, mention_2_tensor,\n",
    "                                         mention_1_tensor * mention_2_tensor], 1)\n",
    "    elif model.use_diff:\n",
    "        mention_pair_tensor = torch.cat([mention_1_tensor, mention_2_tensor,\n",
    "                                         mention_1_tensor - mention_2_tensor], 1)\n",
    "\n",
    "    if config_dict[\"use_binary_feats\"]:\n",
    "        if is_event:\n",
    "            binary_feats = create_args_features_vec(mention_1, mention_2, other_clusters,\n",
    "                                                    device, model)\n",
    "        else:\n",
    "            binary_feats = create_predicates_features_vec(mention_1, mention_2, other_clusters,\n",
    "                                                          device, model)\n",
    "\n",
    "        mention_pair_tensor = torch.cat([mention_pair_tensor,binary_feats], 1)\n",
    "\n",
    "    mention_pair_tensor = mention_pair_tensor.to(device)\n",
    "\n",
    "    return mention_pair_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_event_mentions[0].arg0_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cluster_i = pair_to_merge[0]\n",
    "    cluster_j = pair_to_merge[1]\n",
    "    new_cluster = Cluster(is_event)\n",
    "    new_cluster.mentions.update(cluster_j.mentions)         ##     new_cluster.mentions.update(cluster_j.mentions) : {}\n",
    "    new_cluster.mentions.update(cluster_i.mentions)\n",
    "\n",
    "    keys_pairs_dict = list(curr_pairs_dict.keys())\n",
    "    for pair in keys_pairs_dict:\n",
    "        cluster_pair = (pair[0], pair[1])\n",
    "        if cluster_i in cluster_pair or cluster_j in cluster_pair:\n",
    "            del curr_pairs_dict[pair]\n",
    "\n",
    "    clusters.remove(cluster_i)\n",
    "    clusters.remove(cluster_j)\n",
    "    clusters.append(new_cluster)\n",
    "\n",
    "    if is_event:\n",
    "        lex_vec = create_event_cluster_bow_lexical_vec(new_cluster, model, device, config_dict=config_dict)\n",
    "    else:\n",
    "        lex_vec = create_entity_cluster_bow_lexical_vec(new_cluster, model, device, config_dict=config_dict)\n",
    "\n",
    "    new_cluster.lex_vec = lex_vec\n",
    "\n",
    "    # create arguments features for the new cluster\n",
    "    update_args_feature_vectors([new_cluster], other_clusters, model, device, is_event)\n",
    "\n",
    "    new_pairs = []\n",
    "    for cluster in clusters:\n",
    "        if cluster != new_cluster:\n",
    "            new_pairs.append((cluster, new_cluster))\n",
    "\n",
    "    # create scores for the new pairs\n",
    "    for pair in new_pairs:\n",
    "        pair_score = assign_score(pair, model, device, config_dict=config_dict, is_event=is_event, other_clusters=other_clusters)\n",
    "        curr_pairs_dict[pair] = pair_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 29_ecbplus:\n",
      "Topic 14_ecbplus:\n",
      "Topic 11_ecb:\n",
      "Topic 10_ecbplus:\n",
      "Topic 13_ecbplus:\n",
      "Topic 32_ecb:\n",
      "Topic 20_ecb:\n",
      "Topic 10_ecb:\n",
      "Topic 22_ecb:\n",
      "Topic 4_ecbplus:\n",
      "Topic 33_ecb:\n",
      "Topic 16_ecb:\n",
      "Topic 27_ecbplus:\n",
      "Topic 24_ecbplus:\n",
      "Topic 25_ecbplus:\n",
      "Topic 20_ecbplus:\n",
      "Topic 30_ecb:\n",
      "Topic 24_ecb:\n",
      "Topic 30_ecbplus:\n",
      "Topic 27_ecb:\n",
      "Topic 19_ecbplus:\n",
      "Topic 26_ecbplus:\n",
      "Topic 28_ecbplus:\n",
      "Topic 4_ecb:\n",
      "Topic 26_ecb:\n",
      "Topic 9_ecbplus:\n",
      "Topic 9_ecb:\n",
      "Topic 7_ecb:\n",
      "Topic 13_ecb:\n",
      "Topic 32_ecbplus:\n",
      "Topic 8_ecbplus:\n",
      "Topic 31_ecb:\n",
      "Topic 28_ecb:\n",
      "Topic 6_ecbplus:\n",
      "Topic 33_ecbplus:\n",
      "Topic 1_ecb:\n",
      "Topic 16_ecbplus:\n",
      "Topic 19_ecb:\n",
      "Topic 7_ecbplus:\n",
      "Topic 22_ecbplus:\n",
      "Topic 11_ecbplus:\n",
      "Topic 29_ecb:\n",
      "Topic 1_ecbplus:\n",
      "Topic 6_ecb:\n",
      "Topic 8_ecb:\n",
      "Topic 3_ecbplus:\n",
      "Topic 14_ecb:\n",
      "Topic 25_ecb:\n",
      "Topic 31_ecbplus:\n",
      "Topic 3_ecb:\n"
     ]
    }
   ],
   "source": [
    "for topic_id in topics_keys:\n",
    "    topic = topics[topic_id]\n",
    "\n",
    "    logging.info('=========================================================================')\n",
    "    logging.info('Topic {}:'.format(topic_id))\n",
    "    print('Topic {}:'.format(topic_id))\n",
    "\n",
    "    # init event and entity clusters\n",
    "    event_mentions, entity_mentions = topic_to_mention_list(topic, is_gold=True)\n",
    "\n",
    "    if config_dict[\"train_init_wd_entity_with_gold\"]:\n",
    "        # initialize entity clusters with gold within document entity coreference chains\n",
    "        wd_entity_clusters = create_gold_wd_clusters_organized_by_doc(entity_mentions, is_event=False)\n",
    "    else:\n",
    "        # initialize entity clusters with within document entity coreference system output\n",
    "        wd_entity_clusters = init_entity_wd_clusters(entity_mentions, doc_to_entity_mentions)\n",
    "\n",
    "    entity_clusters = []\n",
    "    for doc_id, clusters in wd_entity_clusters.items():\n",
    "        entity_clusters.extend(clusters)\n",
    "\n",
    "    event_clusters = init_cd(event_mentions, is_event=True)\n",
    "\n",
    "    # initialize cluster representation\n",
    "    update_lexical_vectors(entity_clusters, cd_entity_model, device, config_dict=config_dict, is_event=False)\n",
    "    update_lexical_vectors(event_clusters, cd_event_model, device, config_dict=config_dict, is_event=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
